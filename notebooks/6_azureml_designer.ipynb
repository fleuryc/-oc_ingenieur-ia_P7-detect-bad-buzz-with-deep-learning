{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AzureML Studio : Designer\n",
    "\n",
    "In this notebook, we will use the AzureML Studio's [Designer](https://docs.microsoft.com/en-us/azure/machine-learning/concept-designer) to design our data processing pipeline.\n",
    "\n",
    "The experiment is visible in the AzureML Studio : [oc-p7-automated-ml](https://ml.azure.com/experiments/id/9bde22d7-75e6-41cc-9bfe-03e15873f292?wsid=/subscriptions/da2e4791-6dd1-422b-848a-a961cef6ab89/resourcegroups/OC_P7/workspaces/oc-p7-ml-workspace&tid=43204f6d-c600-4585-985a-6bafda08d2bb)\n",
    "\n",
    "![AzureML Designer - Pipeline](img/azureml_designer_pipeline.png)\n",
    "\n",
    "We will compare this pre-trained local model to the baseline model from [1_baseline.ipynb](1_baseline.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing\n",
    "\n",
    "Before training our models, the data is prepared as follow :\n",
    "- data is sampled to 2% of the original data (stratified according to the target variable)\n",
    "- text is processed :\n",
    "  - expand verb contractions\n",
    "  - remove stop words\n",
    "  - use lemmatization\n",
    "  - detect sentences by adding a sentence terminator \"|||\" that can be used by the n-gram features extractor module\n",
    "  - normalize case to lowercase\n",
    "  - remove numbers\n",
    "  - remove non-alphanumeric special characters and replace them with \"|\" character\n",
    "  - remove duplicate characters\n",
    "  - remove email addresses\n",
    "  - remove URLs\n",
    "  - normalize backslashes to slashes\n",
    "  - split tokens on special characters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text vectorization\n",
    "\n",
    "We need to represent the text as a vector of numbers.\n",
    "\n",
    "### Feature Hashing\n",
    "\n",
    "In this version, we will use the [Feature Hashing](https://docs.microsoft.com/en-us/azure/machine-learning/component-reference/feature-hashing) module to extract features from the text, with the following parameters :\n",
    "- Hashing bitsize : 10 => 2^10 = 1024 features\n",
    "- N-grams : 2 => tokens are couple of words\n",
    "\n",
    "### N-Gram Features\n",
    "\n",
    "In this version, we will use the [Extract N-Gram Features from Text](https://docs.microsoft.com/en-us/azure/machine-learning/component-reference/extract-n-gram-features-from-text) module to extract features from the text, with the following parameters :\n",
    "- Hashing bitsize : 10 => 2^10 = 1024 features\n",
    "- N-grams : 2 => tokens are couple of words\n",
    "- Weighting function : TF-IDF Weight => Represents well the relative importance of a term in a specific document, versus the importance of a term in the whole corpus.\n",
    "- Minimum word length : 25\n",
    "- Minimum n-gram document absolute frequency : 5 => avoid rare words\n",
    "- Maximum n-gram document ratio : 1 => do not exclude very frequent tokens\n",
    "- Normalize n-gram feature vectors : True => normalize the vectors to unit length\n",
    "\n",
    "This creates a vocabulary that is specific to our training data and that will be used for testing our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "We train a [Two-Class Logistic Regression](https://docs.microsoft.com/en-us/azure/machine-learning/component-reference/two-class-logistic-regression) model with the following parameters :\n",
    "- Optimization tolerance : 1e-7\n",
    "- L2 regularization weight : 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "The test dataset goes through the same text pre-processing and vectorization steps as the training dataset, before being used to test the model.\n",
    "\n",
    "| Model | Confusion Matrix | AP | Precision Recall Curve | ROC AUC | ROC Curve |\n",
    "|-------|------------------|----|------------------------|---------|-----------|\n",
    "| Feature Hashing | ![Confusion Matrix](img/azureml_designer_feature_hashing_confusion_matrix.png) | 0.663 | ![Precision Recall Curve](img/azureml_designer_feature_hashing_precision_recall_curve.png) | 0.726 | ![ROC Curve](img/azureml_designer_feature_hashing_ROC_curve.png) |\n",
    "| N-Gram Features | ![Confusion Matrix](img/azureml_designer_n-gram_confusion_matrix.png)          | 0.723 | ![Precision Recall Curve](img/azureml_designer_n-gram_precision_recall_curve.png)          | 0.811 | ![ROC Curve](img/azureml_designer_n-gram_ROC_curve.png)          |\n",
    "\n",
    "We can see that the N-Gram Features model performs better than the Feature Hashing model.\n",
    "\n",
    "The performances on the dataset are similar to our baseline model : \n",
    "- Average Precision = 0.723 (baseline = 0.73 , -1%)\n",
    "- ROC AUC = 0.811 (baseline = 0.74 , +9.6%)\n",
    "\n",
    "Unlike our baseline model, this model is quite balanced, just slightly biased towards the _POSITIVE_ class. It is much less biased than our baseline model : it predicted 6.8% (baseline = 35% , -81%) more _POSITIVE_ (3305) messages than _NEGATIVE_ (3095).\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
