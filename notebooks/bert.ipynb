{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 11:07:17.743269: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-01-21 11:07:17.743293: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Downloading and extracting data files...\n",
      "Data files already downloaded.\n",
      ">>> OK.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import custom helper libraries\n",
    "import os\n",
    "import sys\n",
    "\n",
    "src_path = os.path.abspath(os.path.join(\"../src\"))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "import data.helpers as data_helpers\n",
    "import visualization.helpers as viz_helpers\n",
    "\n",
    "# Maths modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# Viz modules\n",
    "import plotly.express as px\n",
    "\n",
    "# Render for export\n",
    "import plotly.io as pio\n",
    "\n",
    "pio.renderers.default = \"notebook\"\n",
    "\n",
    "# Download and unzip CSV files\n",
    "!cd .. && make dataset && cd notebooks\n",
    "# Load data from CSV\n",
    "df = pd.read_csv(\n",
    "    os.path.join(\n",
    "        \"..\", \"data\", \"raw\", \"training.1600000.processed.noemoticon.csv\"\n",
    "    ),\n",
    "    names=[\"target\", \"id\", \"date\", \"flag\", \"user\", \"text\"],\n",
    ")\n",
    "\n",
    "# Reduce memory usage\n",
    "df = data_helpers.reduce_dataframe_memory_usage(df)\n",
    "\n",
    "# Drop useless columns\n",
    "df.drop(columns=[\"id\", \"date\", \"flag\", \"user\"], inplace=True)\n",
    "\n",
    "# Replace target values with labels\n",
    "df.target.replace(\n",
    "    {\n",
    "        0: \"NEGATIVE\",\n",
    "        2: \"NEUTRAL\",\n",
    "        4: \"POSITIVE\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "df.target.replace(\n",
    "    {\n",
    "        \"NEGATIVE\": 0,\n",
    "        \"POSITIVE\": 1,\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "# Sample data for development\n",
    "TEXT_SAMPLE_SIZE = 2000  # <= 0 for all\n",
    "\n",
    "# Sample data\n",
    "if TEXT_SAMPLE_SIZE > 0:\n",
    "    df = data_helpers.balance_sample(df, \"target\", TEXT_SAMPLE_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "# Tokenizers, Stemmers and Lemmatizers\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-cased\")\n",
    "tokenizer_columns = tokenizer.model_input_names\n",
    "label_column = \"target\"\n",
    "\n",
    "\n",
    "# Processed data path\n",
    "processed_data_path = os.path.join(\"..\", \"data\", \"processed\")\n",
    "tokenized_dataset_file_path = os.path.join(\n",
    "    processed_data_path, \"bert_tokenized_as_dataframe_dataset.pkl\"\n",
    ")\n",
    "\n",
    "\n",
    "# if os.path.exists(tokenized_dataset_file_path):\n",
    "#     # Load encoded dataset\n",
    "#     with (open(tokenized_dataset_file_path, \"rb\")) as f:\n",
    "#         X = pickle.load(f)\n",
    "# else:\n",
    "    ## Encode text\n",
    "    # dataset_df = Dataset.from_pandas(df).map(\n",
    "    #     lambda data: tokenizer(\n",
    "    #         data[\"text\"], padding=\"max_length\", truncation=True\n",
    "    #     ),\n",
    "    #     batched=True,\n",
    "    #     num_proc=4,\n",
    "    # ).to_pandas()\n",
    "\n",
    "    # X = [np.array([dataset_df.iloc[x][col] for col in tokenizer_columns]).ravel() for x in tqdm(range(len(dataset_df)))]\n",
    "\n",
    "    # # Save vectorized dataset as pickle\n",
    "    # with open(tokenized_dataset_file_path, \"wb\") as f:\n",
    "    #     pickle.dump(X, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:00<00:00, 6038.08it/s]\n"
     ]
    }
   ],
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    "token_type_ids = []\n",
    "\n",
    "for sent in tqdm(df.text):\n",
    "    bert_inp = tokenizer(\n",
    "        sent,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )\n",
    "    input_ids.append(bert_inp[\"input_ids\"])\n",
    "    attention_masks.append(bert_inp[\"attention_mask\"])\n",
    "    token_type_ids.append(bert_inp[\"token_type_ids\"])\n",
    "\n",
    "input_ids = np.asarray(input_ids)\n",
    "attention_masks = np.array(attention_masks)\n",
    "token_type_ids = np.array(token_type_ids)\n",
    "labels = np.array(df.target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Train-test split\n",
    "(\n",
    "    input_ids_train,\n",
    "    input_ids_test,\n",
    "    attention_masks_train,\n",
    "    attention_masks_test,\n",
    "    token_type_ids_train,\n",
    "    token_type_ids_test,\n",
    "    labels_train,\n",
    "    labels_test,\n",
    ") = train_test_split(\n",
    "    input_ids,\n",
    "    attention_masks,\n",
    "    token_type_ids,\n",
    "    labels,\n",
    "    test_size=0.2,\n",
    "    stratify=labels,\n",
    "    random_state=42,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling model...\n",
      "Fitting model...\n",
      "Epoch 1/10\n",
      "299/320 [===========================>..] - ETA: 3:20 - loss: 7.6615 - accuracy: 0.5092"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertForSequenceClassification\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import TensorBoard, EarlyStopping\n",
    "from keras.metrics import BinaryCrossentropy, SparseCategoricalAccuracy, AUC\n",
    "\n",
    "\n",
    "# Model constants.\n",
    "model_name = \"bert_for_sequence_classification_on_bert_tokenized_text\"\n",
    "\n",
    "results_data_path = os.path.join(\"..\", \"results\")\n",
    "model_file_path = os.path.join(results_data_path, model_name)\n",
    "\n",
    "if os.path.exists(model_file_path):\n",
    "    # Load model\n",
    "    model = load_model(model_file_path)\n",
    "else:\n",
    "    # Define NN model\n",
    "    print(\"Defining model...\")\n",
    "    model = TFBertForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-cased\", num_labels=2\n",
    "    )\n",
    "\n",
    "    # loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    # metric = tf.keras.metrics.SparseCategoricalAccuracy(\"accuracy\")\n",
    "    # optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5, epsilon=1e-08)\n",
    "\n",
    "    # compile NN network\n",
    "    print(\"Compiling model...\")\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=\"adam\",\n",
    "        metrics=[\n",
    "            SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "            # AUC(curve=\"ROC\", name=\"ROC_AUC\"),\n",
    "            # AUC(curve=\"PR\", name=\"AP\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # fit NN model\n",
    "    print(\"Fitting model...\")\n",
    "    model.fit(\n",
    "        [input_ids_train, attention_masks_train, token_type_ids_train],\n",
    "        labels_train,\n",
    "        epochs=10,\n",
    "        batch_size=4,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[\n",
    "            TensorBoard(log_dir=f\"logs/{model.name}\"),\n",
    "            EarlyStopping(monitor=\"val_loss\", patience=2),\n",
    "        ],\n",
    "        workers=4,\n",
    "        use_multiprocessing=True,\n",
    "    )\n",
    "\n",
    "    # Save model\n",
    "    print(\"Saving model...\")\n",
    "    model.save(model_file_path)\n",
    "\n",
    "print(model.summary())\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "985dab4142e640f9d316b4a6ee5dfcc3b3a4782860c49b4d63e46ae9dfb02f20"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
