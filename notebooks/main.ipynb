{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Air Paradis : Detect  bad buzz with deep learning\n",
    "\n",
    "## Context\n",
    "\n",
    "\"Air Paradis\" is an airline company who's marketing department wants to be able to detect quickly \"bad buzz\" on social networks, to be able to anticipate and address issues as fast as possible. They need an AI API that can detect \"bad buzz\" and predict the reason for it.\n",
    "\n",
    "The goal here is to evaluate three approaches to detect \"bad buzz\" :\n",
    "\n",
    "-   cloud managed service : [Azure Cognitive Service for Language - Sentiment analysis](https://docs.microsoft.com/en-us/azure/cognitive-services/language-service/sentiment-opinion-mining/overview)\n",
    "-   simple model : Logistic Regression trained on pre-processed data (stopwords, stemming, lemmatization, ...)\n",
    "-   advanced models : deep learning models (Keras)\n",
    "    -   with word embedding (Gensim : word2vec, Glove, fasttext)\n",
    "    -   with a \"Long short-term memory\" (LSTM) layer\n",
    "    -   with a Bidirectional Encoder Representations from Transformers (BERT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load project modules\n",
    "\n",
    "The helpers functions and project specific code will be placed in `../src/`.\n",
    "\n",
    "We will use the [Python](https://www.python.org/about/gettingstarted/) programming language, and present here the code and results in this [Notebook JupyterLab](https://jupyterlab.readthedocs.io/en/stable/getting_started/overview.html) file.\n",
    "\n",
    "We will use the usual libraries for data exploration, modeling and visualisation :\n",
    "\n",
    "-   [NumPy](https://numpy.org/doc/stable/user/quickstart.html) and [Pandas](https://pandas.pydata.org/docs/user_guide/index.html) : for maths (stats, algebra, ...) and large data manipulation\n",
    "-   [scikit-learn](https://scikit-learn.org/stable/getting_started.html) : for machine learning models training and evaluation\n",
    "-   [Plotly](https://plotly.com/python/getting-started/) : for interactive data visualization\n",
    "\n",
    "We will also use libraries specific to the goals of this project :\n",
    "\n",
    "-   NLP Natural Language Processing\n",
    "    -   [NLTK](https://www.nltk.org/) and [Spacy](https://spacy.io/api) : for text processing\n",
    "    -   [Gensim](https://radimrehurek.com/gensim/auto_examples/index.html) and [pyLDAvis](https://nbviewer.org/github/bmabey/pyLDAvis/blob/master/notebooks/pyLDAvis_overview.ipynb) : for topic modelling and visualisation\n",
    "    -   [Kereas](https://keras.io/) : for deep learning models training and evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import custom helper libraries\n",
    "import os\n",
    "import sys\n",
    "\n",
    "src_path = os.path.abspath(os.path.join(\"../src\"))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "import features.helpers as feat_helpers\n",
    "import data.helpers as data_helpers\n",
    "import visualization.helpers as viz_helpers\n",
    "\n",
    "\n",
    "# Load environment variables from .env file\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "# SECRET = os.getenv(\"SECRET\")\n",
    "\n",
    "\n",
    "# Set up logging\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# System modules\n",
    "import random, pickle\n",
    "\n",
    "\n",
    "# Maths modules\n",
    "import numpy as np\n",
    "from scipy.stats import f_oneway\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Viz modules\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "\n",
    "# Sample data for development\n",
    "TEXT_SAMPLE_SIZE = 10 * 1000  # <= 0 for all\n",
    "\n",
    "\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis (EDA)\n",
    "\n",
    "We are going to load the data and analyse the distribution of each variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "Let's download the data from the [Kaggle - Sentiment140 dataset with 1.6 million tweets](https://www.kaggle.com/kazanova/sentiment140) competition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and unzip CSV files\n",
    "!cd .. && make dataset && cd notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from CSV\n",
    "df = pd.read_csv(\n",
    "    os.path.join(\"..\", \"data\", \"raw\", \"training.1600000.processed.noemoticon.csv\"),\n",
    "    names=[\"target\", \"id\", \"date\", \"flag\", \"user\", \"text\"],\n",
    ")\n",
    "\n",
    "# Reduce memory usage\n",
    "df = data_helpers.reduce_dataframe_memory_usage(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore data\n",
    "\n",
    "Let's display a few examples, find out how many data points are available, what are the variables and what is their distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diaplay number of rows and colmn types\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are _1600000_ rows, each composed of _6_ columns :\n",
    "\n",
    "-   _target_: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n",
    "-   _id_: The id of the tweet ( 2087)\n",
    "-   _date_: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
    "-   _flag_: The query (lyx). If there is no query, then this value is NO_QUERY.\n",
    "-   _user_: the user that tweeted (robotickilldozr)\n",
    "-   _text_: the text of the tweet (Lyx is cool)\n",
    "\n",
    "We are only interrested in the _target_ and _text_ variables. The rest of the columns are not useful for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop useless columns\n",
    "df.drop(columns=[\"id\", \"date\", \"flag\", \"user\"], inplace=True)\n",
    "\n",
    "# Replace target values with labels\n",
    "df.target.replace(\n",
    "    {\n",
    "        0: \"NEGATIVE\",\n",
    "        2: \"NEUTRAL\",\n",
    "        4: \"POSITIVE\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic statistics\n",
    "df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot target distribution\n",
    "viz_helpers.histogram(\n",
    "    df, label_x=\"target\", label_colour=\"target\", title=\"Target distribution\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are exactly as many (800000) _POSITIVE_ tweets as _NEGATIVE_ tweets. There are no _NEUTRAL_ tweets.\n",
    "The problem is well balanced and there will be no bias towards one class during the training of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot text length distribution\n",
    "df[\"text_length\"] = df.text.str.len()\n",
    "\n",
    "p_value = f_oneway(\n",
    "    df.loc[df[\"target\"] == \"NEGATIVE\", \"text_length\"],\n",
    "    df.loc[df[\"target\"] == \"POSITIVE\", \"text_length\"],\n",
    ")[1]\n",
    "\n",
    "viz_helpers.histogram(\n",
    "    df,\n",
    "    label_x=\"text_length\",\n",
    "    label_colour=\"target\",\n",
    "    title=f\"Text length distribution / p-value={p_value:.5f}\",\n",
    "    include_boxplot=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no big difference between the _POSITIVE_ and _NEGATIVE_ tweets, but _NEGATIVE_ tweets are slightly longer than _POSITIVE_ tweets.\n",
    "In both classes, there are two modes : *~45* characters and *138* characters (the maximum allowed at some point).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot word count distribution\n",
    "df[\"word_count\"] = df.text.str.split().str.len()\n",
    "\n",
    "p_value = f_oneway(\n",
    "    df.loc[df[\"target\"] == \"NEGATIVE\", \"word_count\"],\n",
    "    df.loc[df[\"target\"] == \"POSITIVE\", \"word_count\"],\n",
    ")[1]\n",
    "\n",
    "viz_helpers.histogram(\n",
    "    df,\n",
    "    label_x=\"word_count\",\n",
    "    label_colour=\"target\",\n",
    "    title=f\"Word count distribution / p-value={p_value:.5f}\",\n",
    "    include_boxplot=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no big difference between the _POSITIVE_ and _NEGATIVE_ tweets, but _NEGATIVE_ tweets are significatively longer than _POSITIVE_ tweets.\n",
    "In both classes, there are two modes : *~7* words and *~20* words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text analysis\n",
    "\n",
    "We will look more in details at what contains the _text_ variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizers\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Tokenizers, Stemmers and Lemmatizers\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "\n",
    "# Download resources\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Download SpaCy model\n",
    "!python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define tokenizer\n",
    "tokenizer = lambda text: [  # SpaCy Lemmatizer\n",
    "            token.lemma_.lower()\n",
    "            for token in nlp(text)\n",
    "            if token.is_alpha and not token.is_stop\n",
    "        ]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processed data path\n",
    "processed_data_path = os.path.join(\"..\", \"data\", \"processed\")\n",
    "tfidf_dataset_file_path = os.path.join(\n",
    "    processed_data_path, \"tfidf_dataset.pkl\"\n",
    ")\n",
    "tfidf_vocabulary_file_path = os.path.join(\n",
    "    processed_data_path, \"tfidf_vocabulary.pkl\"\n",
    ")\n",
    "\n",
    "if os.path.exists(tfidf_dataset_file_path) and os.path.exists(tfidf_vocabulary_file_path):\n",
    "    # Load vectorized dataset\n",
    "    with (open(tfidf_dataset_file_path, \"rb\")) as f:\n",
    "        X = pickle.load(f)\n",
    "    # Load vocabulary\n",
    "    with (open(tfidf_vocabulary_file_path, \"rb\")) as f:\n",
    "        vocabulary = pickle.load(f)\n",
    "else:\n",
    "    # Define vectorizer\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        strip_accents=\"unicode\",\n",
    "        lowercase=True,\n",
    "        stop_words=stopwords,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    # Vectorize text\n",
    "    X = vectorizer.fit_transform(df.text)\n",
    "\n",
    "    # Get vocabulary\n",
    "    vocabulary = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Save vectorized dataset as pickle\n",
    "    with open(tfidf_dataset_file_path, \"wb\") as f:\n",
    "        pickle.dump(X, f)\n",
    "\n",
    "    # Save vocabulary as pickle\n",
    "    with open(tfidf_vocabulary_file_path, \"wb\") as f:\n",
    "        pickle.dump(vocabulary, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "\n",
    "# Train LSA model\n",
    "n_components = 50\n",
    "lsa = TruncatedSVD(n_components=n_components, random_state=42).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot explained variance ratio of LSA\n",
    "fig = px.line(\n",
    "    x=range(1, n_components + 1),\n",
    "    y=lsa.explained_variance_ratio_,\n",
    "    title=\"Explained variance ratio of LSA\",\n",
    "    labels={\"x\": \"Component\", \"y\": \"Explained variance ratio\"},\n",
    "    markers=True,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensionality\n",
    "X_lsa = lsa.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_lsa, df.target, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "\n",
    "# Define model\n",
    "model = LogisticRegressionCV(\n",
    "    # n_jobs=-1,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_helpers.plot_classifier_results(\n",
    "    model,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    title=\"Train set results\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_helpers.plot_classifier_results(\n",
    "    model,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    title=\"Test set results\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "985dab4142e640f9d316b4a6ee5dfcc3b3a4782860c49b4d63e46ae9dfb02f20"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
